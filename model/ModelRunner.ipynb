{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"156KMUxDRTYINiNT-vsfqkaExDzrWNHqK","authorship_tag":"ABX9TyODI3T7MC03EZC9wu+CR7eQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"f35LEQibr1Bq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740983171403,"user_tz":-330,"elapsed":47611,"user":{"displayName":"Aditya Bhadra","userId":"05269780662276270975"}},"outputId":"ae1f6910-5229-45c6-db10-e7e33fe8cb43"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","{}\n","\n","### Input:\n","{}\n","\n","### Response:\n","{}\"\"\""],"metadata":{"id":"7nV5uecomINs","executionInfo":{"status":"ok","timestamp":1740983171406,"user_tz":-330,"elapsed":6,"user":{"displayName":"Aditya Bhadra","userId":"05269780662276270975"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["%%capture\n","!pip install unsloth\n","# Also get the latest nightly Unsloth!\n","!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"],"metadata":{"id":"AV8c2LPcqI5U","executionInfo":{"status":"ok","timestamp":1740983343692,"user_tz":-330,"elapsed":172291,"user":{"displayName":"Aditya Bhadra","userId":"05269780662276270975"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"id":"2eSvM9zX_2d3","executionInfo":{"status":"error","timestamp":1740983354131,"user_tz":-330,"elapsed":28,"user":{"displayName":"Aditya Bhadra","userId":"05269780662276270975"}},"colab":{"base_uri":"https://localhost:8080/","height":332},"outputId":"5e57d3e5-c014-4b9a-ca52-f0b1e155b43f"},"outputs":[{"output_type":"error","ename":"NotImplementedError","evalue":"Unsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-3ad1013ef689>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   model, tokenizer = FastLanguageModel.from_pretrained(\n\u001b[1;32m      4\u001b[0m       \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/LexBot/LexBot\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# YOUR MODEL YOU USED FOR TRAINING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;31m# First check if CUDA is available ie a NVIDIA GPU is seen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;31m# Fix Xformers performance issues since 0.0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotImplementedError\u001b[0m: Unsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!"]}],"source":["if True:\n","  from unsloth import FastLanguageModel\n","  model, tokenizer = FastLanguageModel.from_pretrained(\n","      model_name = \"/content/drive/MyDrive/LexBot/LexBot\", # YOUR MODEL YOU USED FOR TRAINING\n","      max_seq_length = 2048,\n","      dtype = None,\n","      load_in_4bit = True,\n","  )\n","  FastLanguageModel.for_inference(model) # Enable native 2x faster inference"]},{"cell_type":"code","source":["inputs = tokenizer(\n","[\n","    alpaca_prompt.format(\n","        \"Given the following conversation, relevant context, and a follow up question, reply with an answer to the current question the user is asking. Return only your response to the question given the above information following the users instructions as needed. Explain elobrately accordind to indan penal code or articales, make the response big with detailed explanation.\", # instruction\n","        \"What is the maximum penalty for drink and drive\", # input\n","        \"\", # output - leave this blank for generation!\n","    )\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 512)\n","print()"],"metadata":{"id":"lHFbweqeNm7n","executionInfo":{"status":"aborted","timestamp":1740983346217,"user_tz":-330,"elapsed":7,"user":{"displayName":"Aditya Bhadra","userId":"05269780662276270975"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import TextStreamer\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","conversation_history = []\n","text_streamer = TextStreamer(tokenizer)\n","\n","while True:\n","    question = input(\"Enter your question: \")\n","    conversation_history.append(f\"User: {question}\")\n","    history_text = \"\\n\".join(conversation_history)\n","\n","    prompt = alpaca_prompt.format(\n","        \"Given the following conversation, relevant context, and a follow-up question, reply with an answer to the current question the user is asking. Return only your response following the user's instructions as needed. Explain elaborately according to the Indian Penal Code or relevant articles, providing a detailed and extensive explanation.\",\n","        f\"Conversation so far:\\n{history_text}\\n\\nExplain in the context of INDIAN LAW: {question}\",\n","        \"\"  # Leave this blank for generation\n","    )\n","\n","    inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n","    output_tokens = model.generate(**inputs, max_new_tokens=512)\n","\n","    response = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n","    response = response.split(\"### Response:\\n\")[-1].strip()\n","\n","    conversation_history.append(f\"Assistant: {response}\")\n","\n","    # Print only the AI's response\n","    print(\"\\nAI:\", response)\n","    print(\"\\n\")  # Extra spacing for readability\n"],"metadata":{"id":"8CQz8YSvggAM","executionInfo":{"status":"aborted","timestamp":1740983346221,"user_tz":-330,"elapsed":1,"user":{"displayName":"Aditya Bhadra","userId":"05269780662276270975"}}},"execution_count":null,"outputs":[]}]}